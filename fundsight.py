# -*- coding: utf-8 -*-
"""FundSight.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CCrlsHMsgEphyVY2R8RTR2nPMTaJemEM
"""

from google.colab import files
uploaded = files.upload()

import zipfile
import os

# Unzip the file
with zipfile.ZipFile("archive.zip", 'r') as zip_ref:
    zip_ref.extractall("fundsight_data")

# Check extracted files
os.listdir("fundsight_data")

import pandas as pd

import zipfile
import os

with zipfile.ZipFile("/content/archive.zip", 'r') as zip_ref:
    zip_ref.extractall("/content/fundsight_data")

# Check what files were extracted
os.listdir("/content/fundsight_data")

import os
os.listdir("/content/fundsight_data/StartUp_FundingScrappingData/2021")

import pandas as pd
import os

folder_path = "/content/fundsight_data/StartUp_FundingScrappingData/2021"

# Read and combine all monthly CSVs
df_list = []
for file in os.listdir(folder_path):
    if file.endswith(".csv"):
        df_month = pd.read_csv(os.path.join(folder_path, file))
        df_month["source_file"] = file  # optional: track which month it came from
        df_list.append(df_month)

df = pd.concat(df_list, ignore_index=True)
df.shape, df.head()

import pandas as pd
import os

base_path = "/content/fundsight_data/StartUp_FundingScrappingData"
all_data = []

# Loop through each year folder (e.g., 2015 to 2021)
for year in os.listdir(base_path):
    year_folder = os.path.join(base_path, year)
    if os.path.isdir(year_folder):
        for file in os.listdir(year_folder):
            if file.endswith(".csv"):
                file_path = os.path.join(year_folder, file)
                df_year = pd.read_csv(file_path)
                df_year["source_file"] = file
                df_year["year"] = year  # add year column
                all_data.append(df_year)

# Combine all years
df_all = pd.concat(all_data, ignore_index=True)
df_all.columns = df_all.columns.str.strip().str.lower().str.replace(' ', '_')
df_all.shape, df_all.head()

df_all.columns = df_all.columns.str.strip().str.lower().str.replace(' ', '_').str.replace(r'\(.*\)', '', regex=True)
df_all.columns

df_all = df_all.loc[:, ~df_all.columns.str.match(r'^\d+$')]

df_all.columns.tolist()

df_all.columns = df_all.columns.str.strip().str.lower().str.replace(' ', '_').str.replace(r'\(.*\)', '', regex=True)

print(df_all.columns.tolist())

df_all['amount'] = df_all['amount'].astype(str) \
    .str.replace(',', '') \
    .str.replace('â‚¹', '') \
    .str.replace('$', '') \
    .str.strip()

df_all['amount'] = pd.to_numeric(df_all['amount'], errors='coerce')

print(df_all[['amount_', 'amount']].head(10))

df_all['amount'] = df_all['amount_'].astype(str) \
    .str.replace(',', '') \
    .str.replace('â‚¹', '') \
    .str.replace('$', '') \
    .str.strip()

df_all['amount'] = pd.to_numeric(df_all['amount'], errors='coerce')

df_all.drop(columns=['amount_'], inplace=True, errors='ignore')

import plotly.express as px

funding_by_year = df_all.groupby('year')['amount'].sum().reset_index()

fig1 = px.bar(funding_by_year, x='year', y='amount',
              title='ðŸ’¸ Total Startup Funding by Year (2015â€“2021)',
              labels={'amount': 'Funding (USD)'})
fig1.show()

df_all['year'].value_counts(dropna=False).sort_index()

df_all[df_all['amount'].notna()].groupby('year')['amount'].sum().sort_index()

df_all.columns.value_counts()

import pandas as pd
import os

base_path = "/content/fundsight_data/StartUp_FundingScrappingData"
all_data = []

for year in os.listdir(base_path):
    year_folder = os.path.join(base_path, year)
    if os.path.isdir(year_folder):
        for file in os.listdir(year_folder):
            if file.endswith(".csv"):
                file_path = os.path.join(year_folder, file)
                df = pd.read_csv(file_path)

                # Normalize column names
                df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace(r'[^\w\s]', '', regex=True)

                # Try to find an 'amount' column (handle different names)
                possible_amount_cols = [col for col in df.columns if 'amount' in col]
                if possible_amount_cols:
                    amt_col = possible_amount_cols[0]
                    df['amount'] = df[amt_col].astype(str) \
                                        .str.replace(',', '') \
                                        .str.replace('â‚¹', '') \
                                        .str.replace('$', '') \
                                        .str.strip()
                    df['amount'] = pd.to_numeric(df['amount'], errors='coerce')
                else:
                    df['amount'] = None  # if no amount found

                # Parse date
                date_cols = [col for col in df.columns if 'date' in col]
                if date_cols:
                    df['date'] = pd.to_datetime(df[date_cols[0]], errors='coerce', dayfirst=True)
                    df['year'] = df['date'].dt.year
                else:
                    df['year'] = year

                df['source_file'] = file
                df['source_year'] = year
                all_data.append(df)

# Combine all years
df_all = pd.concat(all_data, ignore_index=True)
df_all = df_all[df_all['amount'].notna()]  # keep only rows with clean amount
df_all['year'] = df_all['year'].astype('Int64')

df_all[['startup_name', 'amount', 'year']].head()

df_all['year'].value_counts().sort_index()

df_all.groupby('year')['amount'].sum().sort_index()

df_all = df_all[df_all['year'] != 1970]

print(df_all.columns.tolist())

import pandas as pd

# Step 1: Create 'final_amount' as a copy of 'amount'
df_all['final_amount'] = df_all['amount']

# Step 2: Set 'final_amount' column to object type to safely hold strings before conversion
df_all['final_amount'] = df_all['final_amount'].astype(object)

# Step 3: Create a mask for rows where 'final_amount' is NaN and 'amount_in_usd' is not NaN
mask = df_all['final_amount'].isna() & df_all['amount_in_usd'].notna()

# Step 4: Clean and fill the missing values from 'amount_in_usd'
df_all.loc[mask, 'final_amount'] = (
    df_all.loc[mask, 'amount_in_usd'].astype(str)
        .str.replace(',', '')
        .str.replace('â‚¹', '')
        .str.replace('$', '')
        .str.strip()
)

# Step 5: Convert 'final_amount' column to numeric (float), coercing invalid strings to NaN
df_all['final_amount'] = pd.to_numeric(df_all['final_amount'], errors='coerce')

df_all['final_amount'].describe()

import matplotlib.pyplot as plt

df_all.groupby('year')['final_amount'].sum().sort_index().plot(kind='bar', figsize=(10,5))
plt.title("Total Investment Amount by Year")
plt.ylabel("Investment (USD)")
plt.xlabel("Year")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

df_all = df_all[df_all['year'] != 1970]

import numpy as np

yearly = df_all.groupby('year')['final_amount'].sum().sort_index()
ax = yearly.plot(kind='bar', figsize=(10,5))

for i, v in enumerate(yearly):
    ax.text(i, v + 1e8, f"{v/1e9:.2f}B", ha='center', va='bottom')

plt.title("Total Investment Amount by Year")
plt.ylabel("Investment (USD)")
plt.xlabel("Year")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import seaborn as sns

plt.figure(figsize=(10,5))
sns.barplot(x=yearly.index, y=yearly.values, palette='Blues_d')
plt.title("Total Investment Amount by Year")
plt.ylabel("Investment (USD)")
plt.xlabel("Year")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np

# Keep only columns weâ€™ll actually use
keep_cols = [
    'startup_name','final_amount','year','date',
    'city','city__location',
    'industry_vertical','industryvertical','industry/_vertical','subvertical','sub-vertical',
    'investors','investors_name','investor_name',
    'investment_type','investmenttype','investment_stage','remarks','source_file','source_year'
]
use_cols = [c for c in keep_cols if c in df_all.columns]
df = df_all[use_cols].copy()

# Canonical text columns
def first_nonnull(series, prefer=None):
    # prefer: list of columns to prioritize when choosing first non-null
    if prefer is None: return series.bfill(axis=1).iloc[:,0]
    return series[prefer].bfill(axis=1).iloc[:,0]

df['sector'] = first_nonnull(
    df[[c for c in ['industry_vertical','industryvertical','industry/_vertical'] if c in df.columns]],
    prefer=[c for c in ['industry_vertical','industry/_vertical','industryvertical'] if c in df.columns]
)

df['city_clean'] = first_nonnull(
    df[[c for c in ['city','city__location'] if c in df.columns]],
    prefer=[c for c in ['city','city__location'] if c in df.columns]
)

df['investors_clean'] = first_nonnull(
    df[[c for c in ['investors','investors_name','investor_name'] if c in df.columns]],
    prefer=[c for c in ['investors','investors_name','investor_name'] if c in df.columns]
)

df['stage'] = first_nonnull(
    df[[c for c in ['investment_stage'] if c in df.columns]]
) if 'investment_stage' in df.columns else np.nan

df['type'] = first_nonnull(
    df[[c for c in ['investment_type','investmenttype'] if c in df.columns]],
    prefer=[c for c in ['investment_type','investmenttype'] if c in df.columns]
)

# Normalize strings
for col in ['startup_name','sector','city_clean','investors_clean','stage','type']:
    if col in df.columns:
        df[col] = (df[col]
                   .astype(str)
                   .str.strip()
                   .str.replace(r'\s+', ' ', regex=True)
                   .str.title()
                   .replace({'Nan':'', 'None':''}))

# Drop obvious junk
df = df[df['final_amount'].notna() & (df['final_amount'] > 0)]
df = df[df['year'].between(2011, 2021)]  # adjust range as you like

# Save a cleaned version (nice for your repo)
clean_path = '/content/FundSight_clean.csv'
df[['startup_name','final_amount','year','date','city_clean','sector','investors_clean','stage','type','source_file','source_year']].to_csv(clean_path, index=False)
clean_path

total_usd = df['final_amount'].sum()
num_deals = len(df)
avg_deal = df['final_amount'].mean()
years = df['year'].nunique()

kpis = {
    "Total Funding (USD)": f"${total_usd:,.0f}",
    "Number of Deals": f"{num_deals:,}",
    "Average Deal Size": f"${avg_deal:,.0f}",
    "Years Covered": years
}
kpis

import matplotlib.pyplot as plt

yearly = df.groupby('year', as_index=False)['final_amount'].sum()

plt.figure(figsize=(10,5))
ax = plt.bar(yearly['year'], yearly['final_amount'])
plt.title("Total Investment Amount by Year (2011â€“2021)")
plt.ylabel("Investment (USD)")
plt.xlabel("Year")
plt.xticks(yearly['year'], rotation=45)
# annotate
for x, y in zip(yearly['year'], yearly['final_amount']):
    plt.text(x, y*1.01, f"{y/1e9:.2f}B", ha='center', va='bottom', fontsize=9)
plt.tight_layout()
plt.show()

yearly.sort_values('final_amount', ascending=False).head(10)

TOPN = 10

top_startups = (df.groupby('startup_name', as_index=False)['final_amount']
                  .sum().sort_values('final_amount', ascending=False).head(TOPN))
top_sectors  = (df.groupby('sector', as_index=False)['final_amount']
                  .sum().sort_values('final_amount', ascending=False).head(TOPN))
top_cities   = (df.groupby('city_clean', as_index=False)['final_amount']
                  .sum().sort_values('final_amount', ascending=False).head(TOPN))

# investors: split by comma and explode
inv = df[['investors_clean','final_amount']].dropna()
inv = (inv.assign(investor=inv['investors_clean'].str.split(','))
          .explode('investor'))
inv['investor'] = inv['investor'].str.strip().str.title()
top_investors = (inv.groupby('investor', as_index=False)['final_amount']
                   .sum().sort_values('final_amount', ascending=False).head(TOPN))

top_startups, top_sectors, top_cities, top_investors[:10]

def barh(df_, x, y, title):
    plt.figure(figsize=(8,5))
    plt.barh(df_[x], df_[y])
    plt.gca().invert_yaxis()
    plt.title(title)
    plt.xlabel('Investment (USD)')
    plt.tight_layout()
    plt.show()

barh(top_startups, 'startup_name', 'final_amount', 'Top 10 Funded Startups')
barh(top_sectors,  'sector',       'final_amount', 'Top 10 Sectors By Funding')
barh(top_cities,   'city_clean',   'final_amount', 'Top Cities By Funding')
barh(top_investors,'investor',     'final_amount', 'Top Investors By Funding')

by_year = (df.groupby('year')
             .agg(total_funding=('final_amount','sum'),
                  deals=('final_amount','size'),
                  avg_deal=('final_amount','mean'))
             .reset_index())
by_year

# Funding concentration index (Herfindahl) by year across startups
def hhi(x):
    # shares over startups in that year
    shares = (x / x.sum())**2
    return shares.sum()

conc = (df.groupby(['year','startup_name'])['final_amount'].sum()
          .groupby('year').apply(hhi).reset_index(name='hhi_startup_concentration'))

# Volatility of monthly funding (how spiky a year was)
df['month'] = pd.to_datetime(df['date'], errors='coerce').dt.to_period('M')
vol = (df.dropna(subset=['month'])
         .groupby('month')['final_amount'].sum()
         .reset_index()
         .assign(year=lambda d: d['month'].dt.year)
         .groupby('year')['final_amount'].std()
         .reset_index(name='monthly_funding_volatility'))

risk_summary = by_year.merge(conc, on='year', how='left').merge(vol, on='year', how='left')
risk_summary.sort_values('year')

import matplotlib.pyplot as plt

rs = risk_summary.sort_values('year').copy()

fig, ax = plt.subplots(figsize=(9,4))
ax.plot(rs['year'], rs['hhi_startup_concentration'], marker='o')
ax.set_title('Funding Concentration (HHI) by Year')
ax.set_xlabel('Year'); ax.set_ylabel('HHI (0â€“1, higher = more concentrated)')
ax.grid(True); plt.tight_layout(); plt.show()

fig, ax = plt.subplots(figsize=(9,4))
ax.plot(rs['year'], rs['monthly_funding_volatility'], marker='o')
ax.set_title('Monthly Funding Volatility by Year')
ax.set_xlabel('Year'); ax.set_ylabel('Std. Dev. of Monthly Funding (USD)')
ax.grid(True); plt.tight_layout(); plt.show()

from sklearn.preprocessing import MinMaxScaler
import numpy as np

score_df = rs[['year','hhi_startup_concentration','monthly_funding_volatility']].copy()

# handle any NaNs
score_df[['hhi_startup_concentration','monthly_funding_volatility']] = \
    score_df[['hhi_startup_concentration','monthly_funding_volatility']].fillna(0)

scaler = MinMaxScaler()
score_df[['hhi_norm','vol_norm']] = scaler.fit_transform(
    score_df[['hhi_startup_concentration','monthly_funding_volatility']]
)

# weight HHI a bit more than volatility (e.g., 0.6/0.4). tweak if you want.
score_df['risk_score'] = 0.6*score_df['hhi_norm'] + 0.4*score_df['vol_norm']

score_df.sort_values('year')

fig, ax = plt.subplots(figsize=(9,4))
ax.bar(score_df['year'], score_df['risk_score'])
ax.set_title('Composite Funding Risk Score by Year (0â€“1)')
ax.set_xlabel('Year'); ax.set_ylabel('Risk Score')
plt.tight_layout(); plt.show()

# merge back totals to have context
summary_for_story = (by_year[['year','total_funding','deals','avg_deal']]
                     .merge(score_df[['year','risk_score']], on='year', how='left')
                     .sort_values('year'))

peak_year = summary_for_story.sort_values('total_funding', ascending=False).iloc[0]
risky_year = summary_for_story.sort_values('risk_score', ascending=False).iloc[0]
stable_year = summary_for_story.sort_values('risk_score', ascending=True).iloc[0]

insights = {
    "Peak Funding Year": f"{int(peak_year.year)} â€” ${peak_year.total_funding:,.0f}",
    "Highest Risk Year": f"{int(risky_year.year)} â€” score {risky_year.risk_score:.2f}",
    "Most Stable Year": f"{int(stable_year.year)} â€” score {stable_year.risk_score:.2f}",
    "Avg Deal (Peak Year)": f"${peak_year.avg_deal:,.0f} across {int(peak_year.deals)} deals"
}
insights

story = (
    f"In {int(peak_year.year)}, total funding peaked at ${peak_year.total_funding:,.0f}. "
    f"Funding concentration and monthly volatility combine to give a composite risk score. "
    f"The year with the highest composite risk was {int(risky_year.year)} (score {risky_year.risk_score:.2f}), "
    f"while {int(stable_year.year)} appeared most stable (score {stable_year.risk_score:.2f}). "
    f"In the peak year, average deal size was ${peak_year.avg_deal:,.0f} across {int(peak_year.deals)} deals."
)
story

risk_summary_out = summary_for_story.merge(
    rs[['year','hhi_startup_concentration','monthly_funding_volatility']], on='year', how='left'
).sort_values('year')

risk_csv = '/content/FundSight_risk_summary.csv'
risk_summary_out.to_csv(risk_csv, index=False)
risk_csv

import matplotlib.pyplot as plt

rs = risk_summary.sort_values('year').copy()

fig, ax = plt.subplots(figsize=(9,4))
ax.plot(rs['year'], rs['hhi_startup_concentration'], marker='o')
ax.set_title('Funding Concentration (HHI) by Year')
ax.set_xlabel('Year'); ax.set_ylabel('HHI (0â€“1, higher = more concentrated)')
ax.grid(True); plt.tight_layout(); plt.show()

fig, ax = plt.subplots(figsize=(9,4))
ax.plot(rs['year'], rs['monthly_funding_volatility'], marker='o')
ax.set_title('Monthly Funding Volatility by Year')
ax.set_xlabel('Year'); ax.set_ylabel('Std. Dev. of Monthly Funding (USD)')
ax.grid(True); plt.tight_layout(); plt.show()

from sklearn.preprocessing import MinMaxScaler
import numpy as np

score_df = rs[['year','hhi_startup_concentration','monthly_funding_volatility']].copy()

# handle any NaNs
score_df[['hhi_startup_concentration','monthly_funding_volatility']] = \
    score_df[['hhi_startup_concentration','monthly_funding_volatility']].fillna(0)

scaler = MinMaxScaler()
score_df[['hhi_norm','vol_norm']] = scaler.fit_transform(
    score_df[['hhi_startup_concentration','monthly_funding_volatility']]
)

# weight HHI a bit more than volatility (e.g., 0.6/0.4). tweak if you want.
score_df['risk_score'] = 0.6*score_df['hhi_norm'] + 0.4*score_df['vol_norm']

score_df.sort_values('year')

fig, ax = plt.subplots(figsize=(9,4))
ax.bar(score_df['year'], score_df['risk_score'])
ax.set_title('Composite Funding Risk Score by Year (0â€“1)')
ax.set_xlabel('Year'); ax.set_ylabel('Risk Score')
plt.tight_layout(); plt.show()

# merge back totals to have context
summary_for_story = (by_year[['year','total_funding','deals','avg_deal']]
                     .merge(score_df[['year','risk_score']], on='year', how='left')
                     .sort_values('year'))

peak_year = summary_for_story.sort_values('total_funding', ascending=False).iloc[0]
risky_year = summary_for_story.sort_values('risk_score', ascending=False).iloc[0]
stable_year = summary_for_story.sort_values('risk_score', ascending=True).iloc[0]

insights = {
    "Peak Funding Year": f"{int(peak_year.year)} â€” ${peak_year.total_funding:,.0f}",
    "Highest Risk Year": f"{int(risky_year.year)} â€” score {risky_year.risk_score:.2f}",
    "Most Stable Year": f"{int(stable_year.year)} â€” score {stable_year.risk_score:.2f}",
    "Avg Deal (Peak Year)": f"${peak_year.avg_deal:,.0f} across {int(peak_year.deals)} deals"
}
insights

story = (
    f"In {int(peak_year.year)}, total funding peaked at ${peak_year.total_funding:,.0f}. "
    f"Funding concentration and monthly volatility combine to give a composite risk score. "
    f"The year with the highest composite risk was {int(risky_year.year)} (score {risky_year.risk_score:.2f}), "
    f"while {int(stable_year.year)} appeared most stable (score {stable_year.risk_score:.2f}). "
    f"In the peak year, average deal size was ${peak_year.avg_deal:,.0f} across {int(peak_year.deals)} deals."
)
story

risk_summary_out = summary_for_story.merge(
    rs[['year','hhi_startup_concentration','monthly_funding_volatility']], on='year', how='left'
).sort_values('year')

risk_csv = '/content/FundSight_risk_summary.csv'
risk_summary_out.to_csv(risk_csv, index=False)
risk_csv

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# HHI Function
def hhi(x):
    return ((x / x.sum())**2).sum()

# Recompute on filtered data
tmp = df.copy()
tmp['month'] = pd.to_datetime(tmp['date'], errors='coerce').dt.to_period('M')

# Concentration (HHI)
conc = (tmp.groupby(['year','startup_name'])['final_amount'].sum()
          .groupby('year').apply(hhi).reset_index(name='HHI'))

# Volatility
vol = (tmp.dropna(subset=['month'])
         .groupby('month')['final_amount'].sum().reset_index()
         .assign(year=lambda d: d['month'].dt.year)
         .groupby('year')['final_amount'].std()
         .reset_index(name='Volatility'))

# Yearly summary
by_year_f = (tmp.groupby('year')
             .agg(Total=('final_amount','sum'),
                  Deals=('final_amount','size'),
                  AvgDeal=('final_amount','mean'))
             .reset_index())

# Merge
risk = by_year_f.merge(conc, on='year', how='left').merge(vol, on='year', how='left').fillna(0)

# Normalize with MinMaxScaler
scaler = MinMaxScaler()
risk[['HHI_n', 'Vol_n']] = scaler.fit_transform(risk[['HHI', 'Volatility']])
risk['RiskScore'] = 0.6*risk['HHI_n'] + 0.4*risk['Vol_n']

# Show results in notebook
display(risk.sort_values('year'))

import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# --- Recompute risk on your cleaned DF ---
tmp = df.copy()
tmp['date']  = pd.to_datetime(tmp['date'], errors='coerce')
tmp['month'] = tmp['date'].dt.to_period('M')

def hhi(x):
    s = (x/x.sum())**2
    return s.sum()

# HHI by year
conc = (tmp.groupby(['year','startup_name'])['final_amount'].sum()
          .groupby('year').apply(hhi).reset_index(name='HHI'))

# Monthly volatility by year
vol = (tmp.dropna(subset=['month'])
         .groupby('month')['final_amount'].sum().reset_index()
         .assign(year=lambda d: d['month'].dt.year)
         .groupby('year')['final_amount'].std()
         .reset_index(name='Volatility'))

by_year_f = (tmp.groupby('year')
               .agg(Total=('final_amount','sum'),
                    Deals=('final_amount','size'),
                    AvgDeal=('final_amount','mean'))
               .reset_index())

risk = by_year_f.merge(conc, on='year', how='left').merge(vol, on='year', how='left').fillna(0)

# Composite risk (0â€“1)
scaler = MinMaxScaler()
risk[['HHI_n','Vol_n']] = scaler.fit_transform(risk[['HHI','Volatility']])
risk['RiskScore'] = 0.6*risk['HHI_n'] + 0.4*risk['Vol_n']

# ---- Chart
risk_sorted = risk.sort_values('year')
plt.figure(figsize=(9,4))
plt.plot(risk_sorted['year'], risk_sorted['RiskScore'], marker='o')
plt.title('Composite Funding Risk Score by Year (0â€“1)')
plt.xlabel('Year'); plt.ylabel('Risk Score'); plt.grid(True); plt.tight_layout()
plt.show()

risk_sorted

import numpy as np

years = risk_sorted['year'].to_numpy()
scores = risk_sorted['RiskScore'].to_numpy()[None, :]  # 1 x N heat row

plt.figure(figsize=(9,1.8))
plt.imshow(scores, aspect='auto')
plt.yticks([]); plt.xticks(ticks=np.arange(len(years)), labels=years, rotation=45)
plt.title('RiskScore Heat Row (lighter = higher)'); plt.tight_layout(); plt.show()

# 1) Clean dataset you built earlier
df[['startup_name','final_amount','year','date','city_clean','sector',
    'investors_clean','stage','type','source_file','source_year']].to_csv('/content/FundSight_clean.csv', index=False)

# 2) Risk summary table
risk_out = risk_sorted.merge(
    conc[['year','HHI']], on='year', how='left'
).merge(
    vol[['year','Volatility']], on='year', how='left'
)
risk_out.to_csv('/content/FundSight_risk_summary.csv', index=False)

# 3) (optional) smaller sample for faster deploys (last 4 years)
df_sample = df[df['year']>=2018].copy()
df_sample.to_csv('/content/FundSight_clean_sample.csv', index=False)

# Download to your laptop
from google.colab import files
files.download('/content/FundSight_clean.csv')
files.download('/content/FundSight_risk_summary.csv')
files.download('/content/FundSight_clean_sample.csv')